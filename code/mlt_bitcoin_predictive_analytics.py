# -*- coding: utf-8 -*-
"""MLT_Bitcoin_Predictive_Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MBLaEbwH-JbT8Z_IJeAzGc6RYCM9omPi

# **Data Understanding**

## **Data Loading**
"""

!pip install kaggle

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d sudalairajkumar/cryptocurrencypricehistory/

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import zipfile

from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error

# ekstrak file zip
local_zip = '/content/cryptocurrencypricehistory.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

dataseat = pd.read_csv('coin_Bitcoin.csv')
dataseat

"""## **Exploratory Data Analysis**

### **Deskripsi Variabel**

Berdasarkan informasi dari [Kaggle](https://www.kaggle.com/datasets/sudalairajkumar/cryptocurrencypricehistory?select=coin_Bitcoin.csv) variabel dari dataseat adalah sebagai berikut :

* Name : Nama mata uang kripto
* Symbol : Singkatan mata uang kripto
* Date : tanggal pengamatan
* Open : Harga pembukaan pada hari tertentu
* High : Harga tertinggi pada hari tertentu
* Low : Harga terendah pada hari tertentu
* Close : Harga penutupan pada hari tertentu
* Volume : Volume transaksi pada hari tertentu
* Market Cap : Kapitalisasi pasar dalam USD

Setelah memahami deskripsi variabel pada data, langkah selanjutnya adalah mengecek informasi pada dataset
"""

dataseat.info()

"""Dari output terlihat bahwa :
* Terdapat 3 kolom dengan tipe object, yaitu: Name, Symbol, dan Date.
* Terdapat 6 kolom numerik dengan tipe data float64 yaitu: high, low, open, close, volume, dan marketcap.
* Terdapat 1 kolom dengan tipe data int64, yaitu: SNo. Kolom ini hanya nomor urut pada tabel

Uraian di atas menunjukkan bahwa setiap kolom telah memiliki tipe data yang sesuai. Selanjutnya, kita perlu mengecek deskripsi statistik dataseat
"""

dataseat.describe()

"""Uraian diatas memberikan informasi statistik pada masing-masing kolom, antara lain:

* Count  adalah jumlah sampel pada data.
* Mean adalah nilai rata-rata.
* Std adalah standar deviasi.
* Min yaitu nilai minimum setiap kolom. 
* 25% adalah kuartil pertama. Kuartil adalah nilai yang menandai batas interval dalam empat bagian sebaran yang sama. 
* 50% adalah kuartil kedua, atau biasa juga disebut median (nilai tengah).
* 75% adalah kuartil ketiga.
* Max adalah nilai maksimum.

### **Menagani Missing Value**

Tahap selanjutnya kita harus mengecek apakah terdapat missing value pada dataset, karna missing value akan berperngaruh terhadap performa model namtinya
"""

dataseat.isnull().sum()

"""Uraian diatas menunjukan bahwa tidak terdapat missing value pada dataseat

### **Menangani Outlier**

Pada tahap ini kita perlu mengecek outlier pada dataseat. kita akan menggunakan teknik visualisasi, yaitu jenis boxplot.
"""

numeric_features = dataseat.select_dtypes(include=np.number).columns.tolist()
plt.figure(figsize=(15, 8))

for i, col in enumerate(numeric_features):
  plt.subplot(3,3,i+1)
  dataseat.boxplot(column=col)

"""Dari hasil visualisasi diatas dapat dilihat bahwa banyak terdapat banyak outlier pada masing-masing kolom dataseat kecuali kolom SNo karna kolom tersebut hanyalah kolom nomor urut pada dataseat.

Untuk mengatasi oulier tersebut kita akan mengunakan teknik IQR (Inter Quartile Range) methode yaitu dengan menghapus data yang berada diluar interquartile range. Interquartile merupakan range diantara kuartil pertama(25%) dan kuartil ketiga(75%)
"""

Q1 = dataseat.quantile(.25)
Q3 = dataseat.quantile(.75)

IQR = Q3 - Q1

bot_treshold = Q1 - 1.5 * IQR
top_treshold = Q3 + 1.5 * IQR

dataseat = dataseat[~((dataseat < bot_treshold) | (dataseat > top_treshold)).any(axis=1)]
dataseat.shape

"""### **Univariate Analysis**

Karena target prediksi dari dataset ini ada pada fitur Close yang merupakan harga Bitcoin, jadi hanya fokus menganalisis korelasi data pada feature tersebut. Dari hasil visualisasi data dibawah dapat disimpulkan bahwa peningkatan harga bitcoin sebanding dengan penurunan jumlah sampel data.
"""

dataseat.hist(bins=50, figsize=(20, 15))
plt.show()

"""### **Multivariate Analysis**

Jika di lihat dari visualisasi data dibawah. Fitur Close pada sumbu y memiliki korelasi dengan data pada fitur High, Low, Open, dan Marketcap. Korelasi yang terdapat pada data-data tersebut merupakan korelas yang tinggi, sedangkan untuk fitur Volume terlihat memiliki korelasi yang cukup lemah karena sebaran datanya tidak membentuk pola
"""

sns.pairplot(dataseat, diag_kind = 'kde')
plt.show()

"""Untuk lebih jelasnya dapat dilihat melalui visualisasi dibawah yang menunjukkan skor korelasi di tiap fitur dengan fitur Close. Pada fitur High, Low, Open dan Marketcap memiliki skor korelasi yang terbilang tinggi yaitu 1. Sedangkan pada fitur Volume memiliki skor korelasi yang cukup rendah yaitu 0.82. Sehingga fitur Volume ini dapat didrop dari dataset."""

plt.figure(figsize=(10, 8))
correlation_matrix = dataseat.corr().round(2)
 
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""# **Data Preparation**

## **Seleksi Fitur**

Pada tahap ini kita akan melakukan seleksi fitur, kita akan menghapus kolom yang tidak dibuhkan yaitu SNo, Name, Symbol, Date, Volume dan Marketcap. Hal ini dilakukan agar data yang tidak diperlukan tidak menggangu model
"""

unused_columns = ['SNo', 'Name', 'Symbol', 'Date', 'Volume', 'Marketcap']

dataseat.drop(unused_columns, axis=1, inplace=True)
dataseat

"""## **Train-Test-Split**

Pada tahap ini kita akan membagi dataset menjadi data latih (train) dan data uji (test), hal ini merupakan hal yang harus kita lakukan sebelum membuat model.

Proporsi data latih dan data uji yang kita guanakan yaitu 80:20
"""

x = dataseat.drop(['Close'], axis=1).values
y = dataseat['Close'].values

x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=1)

print(f'Total # of sample in whole dataset: {len(x)}')
print(f'Total # of sample in train dataset: {len(x_train)}')
print(f'Total # of sample in test dataset: {len(x_test)}')

"""## **Standarisasi**

Algoritma machine learning memiliki performa lebih baik dan konvergen lebih cepat ketika dimodelkan pada data dengan skala relatif sama atau mendekati distribusi normal. Proses scaling dan standarisasi membantu untuk membuat fitur data menjadi bentuk yang lebih mudah diolah oleh algoritma.

Pada tahap ini kita akan menggunakan teknik MinMaxScaler. MinMaxScaler mentransformasikan fitur dengan menskalakan setiap fitur ke rentang tertentu
"""

scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)

"""# **Model Development**

Pada tahap ini, kita akan mengembangkan model machine learning dengan tiga algoritma. Kemudian melakukan tuning hyperparameters untuk mendapatkan parameter dengan performa terbaik pada model, kita akan mengevaluasi performa masing-masing algoritma dan menentukan algoritma mana yang memberikan hasil prediksi terbaik. Ketiga algoritma yang akan kita gunakan, antara lain:

1. Support Vector Machine
2. K-Nearest Neighbours
3. Random Forest
"""

models = pd.DataFrame(columns=['train', 'test'],
                      index=['SVR', 'KNN', 'RF'])

"""## **Tuning Hyperparameters**

Untuk melakukan tuning hyperparameter pada proyek ini menggunakan teknik Grid search. Grid search memungkinkan untuk menguji beberapa parameter sekaligus pada sebuah model. Dengan menerapkan teknik ini kita dapat melihat performa model terbaik dengan parameter tertentu.

### **Support Vector Machine**
"""

svr = SVR()
parameters = {
    'kernel': ['rbf'],
    'C':     [1000, 10000, 100000],
    'gamma': [0.3, 0.03, 0.003]
}

svr_search = GridSearchCV(
    svr, 
    parameters,
    cv=5, 
    verbose=1,
    n_jobs=6,
)

svr_search.fit(x_train, y_train)
svr_best_params = svr_search.best_params_

"""### **K-Nearest Neighbours**"""

knn = KNeighborsRegressor()
parameters =  {
    'n_neighbors': range(1, 25),
}

knn_search = GridSearchCV(
  knn, 
  parameters, 
  cv=5,
  verbose=1, 
  n_jobs=6,
)

knn_search.fit(x_train, y_train)
knn_best_params = knn_search.best_params_

"""### **Random Forest**"""

rf = RandomForestRegressor()
parameters =  {
    'n_estimators': range(1, 10),
    'max_depth': [16, 32, 64],
}

rf_search = GridSearchCV(
  rf, 
  parameters, 
  cv=5,
  verbose=1,
  n_jobs=6,
)
rf_search.fit(x_train, y_train)
rf_best_params = rf_search.best_params_

"""## **Training Model**

### **Support Vector Machine**
"""

svr = SVR(
  C=svr_best_params["C"], 
  gamma=svr_best_params["gamma"], 
  kernel=svr_best_params['kernel']
)                          
svr.fit(x_train, y_train)

"""### **K-Nearest Neighbours**"""

knn = KNeighborsRegressor(n_neighbors=knn_best_params["n_neighbors"])
knn.fit(x_train, y_train)

"""### **Random Forest**"""

rf = RandomForestRegressor(
  n_estimators=rf_best_params["n_estimators"], 
  max_depth=rf_best_params["max_depth"]
)
rf.fit(x_train, y_train)

"""# **Evaluasi Model**

Pada tahap ini metrik yang akan kita gunakan adalah MSE atau Mean Squared Error yang menghitung jumlah selisih kuadrat rata-rata nilai sebenarnya dengan nilai prediksi

sebelum menghitung nilai MSE dalam model, kita perlu melakukan proses scaling fitur numerik pada data uji. Hal ini harus dilakukan agar skala antara data latih dan data uji sama dan kita bisa melakukan evaluasi.
"""

x_test = scaler.transform(x_test)

"""Setelah melakukan scaling pada data uji kita evaluasi ketiga model kita dengan metrik MSE"""

model_dict = {'SVR': svr, 'KNN': knn, 'RF': rf}

for name, model in model_dict.items():
  models.loc[name, 'train'] = mean_squared_error(
    y_true=y_train, 
    y_pred=model.predict(x_train)
  )
  models.loc[name, 'test'] = mean_squared_error(
    y_true=y_test, 
    y_pred=model.predict(x_test)
  ) 

models

"""Untuk memudahkan, mari kita plot metrik tersebut dengan bar chart"""

fig, ax = plt.subplots()
models.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Dari gambar di atas, terlihat bahwa, model Support Vector Machine (SVM) memberikan nilai eror yang paling kecil. Sedangkan model dengan algoritma K-Nearest Neighbours (KNN) memiliki eror yang paling besar. Model SVR yang akan kita pilih sebagai model terbaik untuk melakukan prediksi harga bitcoin."""

test_data = x_test.copy()
predictions = {'y_true':y_test}
for name, model in model_dict.items():
  predictions[name] = model.predict(test_data)
 
predictions = pd.DataFrame(predictions)
predictions

predictions = predictions.tail(10)
predictions.plot(kind='bar',figsize=(16,10))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()